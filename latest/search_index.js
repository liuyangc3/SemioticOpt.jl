var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"This section documents all functions not documented elsewhere","category":"page"},{"location":"api/","page":"API","title":"API","text":"SemioticOpt.iteration\nSemioticOpt.f\nSemioticOpt.η\nSemioticOpt.hooks\nSemioticOpt.λ\nSemioticOpt.maybeminimize!\nSemioticOpt.postiteration\nSemioticOpt.shouldstop\nSemioticOpt.x\nSemioticOpt.x!\nSemioticOpt.x₀","category":"page"},{"location":"api/#SemioticOpt.iteration","page":"API","title":"SemioticOpt.iteration","text":"iteration(f::Function, a::GradientDescent)\n\nOne iteration of a on the function f.\n\nThis function is unexported.\n\n\n\n\n\niteration(f::Function, a::ProjectedGradientDescent)\n\nApply a to f, projected based on a.t.\n\n\n\n\n\niteration(obj::Function, alg::PairwiseGreedyOpt)\n\nOne iteration of the pairwise greedy optimization algorithm alg for objective obj.\n\nExample\n\njulia> using SemioticOpt\njulia> using LinearAlgebra\njulia> f(x, ixs, a, b) = -((a[ixs] .* x) ./ (x .+ b[ixs])) |> sum\njulia> aa = Float64[1, 1, 1000, 1]\njulia> bb = Float64[1, 1, 1, 1]\njulia> f(x, ixs) = f(x, ixs, aa, bb)\njulia> function makepgd(v)\n           return ProjectedGradientDescent(;\n               x=v,\n               η=1e-1,\n               hooks=[StopWhen((a; kws...) -> norm(SemioticOpt.x(a) - kws[:z]) < 1.0)],\n               t=v -> σsimplex(v, 1)  # Project onto unit-simplex\n           )\n       end\njulia> alg = PairwiseGreedyOpt(;\n           kmax=4,\n           x=zeros(4),\n           xinit=zeros(4),\n           f=f,\n           a=makepgd,\n           hooks=[StopWhen((a; kws...) -> kws[:f](kws[:z]) ≤ kws[:f](SemioticOpt.x(a)))]\n       )\njulia> c = 0.1  # per non-zero cost\njulia> selection = x -> f(x, 1:length(x)) + c * length(SemioticOpt.nonzeroixs(x))\njulia> z = SemioticOpt.iteration(selection, alg)\n4-element Vector{Float64}:\n 0.0\n 0.0\n 1.0\n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.f","page":"API","title":"SemioticOpt.f","text":"Getter for the stop-function.\n\n\n\n\n\nf(g::PairwiseGreedyOpt)\n\nThe objective function of the inner loop.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.η","page":"API","title":"SemioticOpt.η","text":"η(g::GradientDescent)\n\nThe learning rate/step size.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.hooks","page":"API","title":"SemioticOpt.hooks","text":"hooks(g::GradientDescent)\n\nThe hooks used by the algorithm.\n\n\n\n\n\nhooks(g::PairwiseGreedyOpt)\n\nThe hooks used by the algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.λ","page":"API","title":"SemioticOpt.λ","text":"Get the λ of h.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.maybeminimize!","page":"API","title":"SemioticOpt.maybeminimize!","text":"maybeminimize!(f::Function, a::OptAlgorithm, op::Function)\n\nMinimize f using a, which calls op for updating a.x.\n\nThis function may be in-place. Don't use it directly unless you know what you're doing. This function is unexported.\n\nwarning: Warning\nIf you don't provide any hook with the IsStoppingCondition trait, this will loop forever.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.postiteration","page":"API","title":"SemioticOpt.postiteration","text":"postiterationhook(\n    hs::AbstractVecOrTuple{H}, a::OptAlgorithm, z::AbstractVector{T}; locals...\n) where {H<:Hook,T<:Real}\n\nRun hooks that the code should execute after SemioticOpt.iteration.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.shouldstop","page":"API","title":"SemioticOpt.shouldstop","text":"shouldstop(hs::AbstractVecOrTuple{H}, a::OptAlgorithm; locals...)\n\nMap over the hooks hs to check for stopping conditions.\n\nThis performs an OR operation if there are multiple hooks that have the SemioticOpt.IsStoppingCondition trait.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.x","page":"API","title":"SemioticOpt.x","text":"x(g::GradientDescent)\nx(g::GradientDescent, v)\n\nThe current best guess for the solution. If using the setter, v is the new value.\n\nThe setter is not in-place. See SemioticOpt.x!.\n\n\n\n\n\nx(g::ProjectedGradientDescent)\nx(g::ProjectedGradientDescent, v)\n\nThe current best guess for the solution. If using the setter, v is the new value.\n\nThe setter is not in-place. See SemioticOpt.x!.\n\n\n\n\n\nx(g::PairwiseGreedyOpt)\nx(g::PairwiseGreedyOpt, v)\n\nThe current best guess for the solution. If using the setter, v is the new value.\n\nThe setter is not in-place. See SemioticOpt.x!.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.x!","page":"API","title":"SemioticOpt.x!","text":"x!(g::GradientDescent, v)\n\nIn-place setting of g.x to v\n\nSee SemioticOpt.x.\n\n\n\n\n\nx!(g::ProjectedGradientDescent, v)\n\nIn-place setting of a.g.x to v\n\nSee SemioticOpt.x.\n\n\n\n\n\nx!(g::PairwiseGreedyOpt, v)\n\nIn-place setting of g.x to v\n\nSee SemioticOpt.x.\n\n\n\n\n\n","category":"function"},{"location":"api/#SemioticOpt.x₀","page":"API","title":"SemioticOpt.x₀","text":"Get the anchor of h.\n\n\n\n\n\n","category":"function"},{"location":"hooks/#hooks","page":"Hooks","title":"Hooks","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Hooks enable you to dynamically choose to inject functionality into algorithms. Some hooks are mandatory for certain algorithms, such as having a hook with the SemioticOpt.IsStoppingCondition trait when using SemioticOpt.GradientDescent. Others are purely there for you to use at your discretion. In this section, we'll take you through Traits, Using Predefined Hooks and how to Create Custom Hooks.","category":"page"},{"location":"hooks/#Traits","page":"Hooks","title":"Traits","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Hooks have traits. This is how our code knows when to execute which hook. For example, the code will execute a hook that has the SemioticOpt.IsStoppingCondition trait when evaluating whether it has finished optimising. A full list of traits follows.","category":"page"},{"location":"hooks/#StopTrait","page":"Hooks","title":"StopTrait","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"This trait tells the code if it should execute a hook when checking stopping conditions. By default, the code automatically gives all hooks the SemioticOpt.IsNotStoppingCondition trait except in certain situations, which are explicitly documented with the hooks in question. We decide to stop code execution on an OR basis. This means that if any hook with SemioticOpt.IsStoppingCondition returns true, the code breaks out of the optimisation loop. Hooks with this trait must implement SemioticOpt.stophook.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"SemioticOpt.StopTrait\nSemioticOpt.IsStoppingCondition\nSemioticOpt.IsNotStoppingCondition\nSemioticOpt.stophook","category":"page"},{"location":"hooks/#SemioticOpt.StopTrait","page":"Hooks","title":"SemioticOpt.StopTrait","text":"Abstract type for stopping conditions traits\n\n\n\n\n\n","category":"type"},{"location":"hooks/#SemioticOpt.IsStoppingCondition","page":"Hooks","title":"SemioticOpt.IsStoppingCondition","text":"Exhibited by hooks that are stopping conditions.\n\nStopping condition hooks must emit a boolean value when SemioticOpt.stophook is called. If multiple hooks meet IsStoppingCondition are instantiated at the same time, we assume that they are meant to be OR'ed, so if any of them is true, optimisation is finished. For more complex behaviour, consider defining a more complex function using a single StopWhen.\n\nTo set this trait for a hook, run\n\njulia> StopTrait(::Type{MyHook}) = IsStoppingCondition()\n\n\n\n\n\n","category":"type"},{"location":"hooks/#SemioticOpt.IsNotStoppingCondition","page":"Hooks","title":"SemioticOpt.IsNotStoppingCondition","text":"Exhibited by hooks that are not stopping conditions.\n\nThis is the default case. You should not have to set it manually for any hooks.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#SemioticOpt.stophook","page":"Hooks","title":"SemioticOpt.stophook","text":"stophook(::IsStoppingCondition, h::Hook, a::OptAlgorithm; locals...)\n\nRaise an error if the hook is a stopping condition but has not implemented SemioticOpt.stophook.\n\n\n\n\n\nstophook(h::IsNotStoppingCondition, a::OptAlgorithm; locals...)\n\nIf the hook isn't a stopping condition, it shouldn't be considered in the OR, so return false.\n\n\n\n\n\nstophook(::IsStoppingCondition, h::StopWhen, a::OptAlgorithm; locals...)\n\nCall the stop-function on a and ;locals.\n\n\n\n\n\n","category":"function"},{"location":"hooks/#PostIterationTrait","page":"Hooks","title":"PostIterationTrait","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"This trait tells the code if it should execute the hook after it calls SemioticOpt.iteration. All hooks default to the SemioticOpt.DontRunAfterIteration trait unless otherwise documented. Hooks with this positive variant of this trait SemioticOpt.RunAfterIteration must return z, the output of iteration. Hooks with this trait must also implement SemioticOpt.postiterationhook.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"SemioticOpt.PostIterationTrait\nSemioticOpt.RunAfterIteration\nSemioticOpt.DontRunAfterIteration\nSemioticOpt.postiterationhook","category":"page"},{"location":"hooks/#SemioticOpt.PostIterationTrait","page":"Hooks","title":"SemioticOpt.PostIterationTrait","text":"Abstract type for trait denoting hooks that should run after an iteration finishes.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#SemioticOpt.RunAfterIteration","page":"Hooks","title":"SemioticOpt.RunAfterIteration","text":"Exhibited by hooks that run after an iteration finishes.\n\nSuch hooks must take the output of SemioticOpt.iteration z as input and return z back, potentially modified.\n\nTo set this trait for a hook, run\n\njulia> PostIterationTrait(::Type{MyHook}) = RunAfterIteration()\n\n\n\n\n\n","category":"type"},{"location":"hooks/#SemioticOpt.DontRunAfterIteration","page":"Hooks","title":"SemioticOpt.DontRunAfterIteration","text":"Exhibited by hooks that don't run after an iteration finishes.\n\nThis is the default case. You should not have to set it manually for any hooks.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#SemioticOpt.postiterationhook","page":"Hooks","title":"SemioticOpt.postiterationhook","text":"postiterationhook(\n    ::RunAfterIteration, h::Hook, a::OptAlgorithm, z::AbstractVector{T}; locals...\n) where {T<:Real}\n\nRaise an error if the hook exhibits SemioticOpt.RunAfterIteration but has not implemented SemioticOpt.postiterationhook.\n\n\n\n\n\npostiterationhook(     ::DontRunAfterIteration, h::Hook, a::OptAlgorithm, z::AbstractVector{T}; locals... ) where {T<:Real}\n\nIf the hook shouldn't run after an iteration, just return z unmodified.\n\n\n\n\n\n","category":"function"},{"location":"hooks/#Using-Predefined-Hooks","page":"Hooks","title":"Using Predefined Hooks","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Hooks always descend from the same abstract type.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"SemioticOpt.Hook","category":"page"},{"location":"hooks/#SemioticOpt.Hook","page":"Hooks","title":"SemioticOpt.Hook","text":"Abstract type for hooks.\n\n\n\n\n\n","category":"type"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Generally speaking, pre-defined hooks won't exhibit any positive traits unless explicitly documented otherwise. This serves two purposes. Firstly, it prevents any unexpected behaviour when the code executes. Secondly, it gives you more flexibility when choosing where you want a hook to be executed.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"warning: Warning\nThis section is incomplete! We will fill this out more once we have more traits and hooks to work with.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"note: Note\nWe recommend you read the below StopWhen section as it explains details that we won't cover in the later sections since it'd be too repetitive.","category":"page"},{"location":"hooks/#StopWhen","page":"Hooks","title":"StopWhen","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"This hook has the SemioticOpt.IsStoppingCondition trait. To use it, you would specify a function that returns a boolean value. If said value is true, then the code breaks out of the optimisation loop.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Let's take an example from our tests to demonstrate how to specify this hook. We'll also implement a dummy optimisation function that just implements a counter for illustration purposes.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> using SemioticOpt\njulia> struct FakeOptAlg <: SemioticOpt.OptAlgorithm end\njuila> a = FakeOptAlg()\njulia> function counter(h, a)\n            i = 0\n            while !shouldstop(h, a; Base.@locals()...)\n                i += 1\n            end\n            return i\n        end\njulia> h = StopWhen((a; locals...) -> locals[:i] ≥ 5, Dict())  # Stop when i ≥ 5\njulia> i = counter((h,), a)\n5","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"One thing you may not have seen is Base.@locals. This takes variables from the local scope (in this case, from the counter scope), and tracks them as a dictionary of symbols. Thus, since i is a local variable inside of counter, :i becomes a key in the Base.@locals dictionary. We pass this dictionary to the anonymouse function stored by StopWhen. Then, we can use locals[:i] to get the value of i from the counter scope and check it against some condition. This is a powerful trick you may find yourself using a lot when dealing with hooks.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"SemioticOpt.StopWhen","category":"page"},{"location":"hooks/#SemioticOpt.StopWhen","page":"Hooks","title":"SemioticOpt.StopWhen","text":"StopWhen(f::Function)\n\nStops optimisation when some condition is met.\n\nThe condition is set by f. Note that f gets access to variables in the SemioticOpt.minimize scope. This means, for example, that it can use locals[:z] to compute residuals. This has the SemioticOpt.IsStoppingCondition trait.\n\n\n\n\n\n","category":"type"},{"location":"hooks/#Create-Custom-Hooks","page":"Hooks","title":"Create Custom Hooks","text":"","category":"section"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"When you create a custom hook, you need to follow three steps. The first is that you need to descend from SemioticOpt.Hook.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> using SemioticOpt\njulia> struct MyHook <: Hook end","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"The second is that you need to ensure that you specify which traits you want that hook to exhibit. For example, let's say MyHook is a stopping condition. You'd want to implement.","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> StopTrait(::Type{MyHook}) = IsStoppingCondition()","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"Finally, if you need non-default behaviour for when the hook executes, you'll need to implement whatever function(s) the code calls for that trait-type. For IsStoppingCondition, that's stophook. Say we want MyHook to immediately cause optimisation to finish. We'd implement","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"julia> stophook(h::MyHook, a::SemioticOpt.OptAlgorithm; locals...) = true","category":"page"},{"location":"hooks/","page":"Hooks","title":"Hooks","text":"That's it! As long as your follow those three steps, you should be able to implement whatever hook you want!","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SemioticOpt","category":"page"},{"location":"#SemioticOpt","page":"Home","title":"SemioticOpt","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements optimisation algorithms for use within simulation and production. For the most part, your workflow should look something like","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using SemioticOpt\njulia> using LinearAlgebra\njulia> f(x) = sum(x .^ 2)  # Specify function to optimise as min f(x)\njulia> a = GradientDescent(;\n            x=[100.0, 50.0],  # Specify parameters for optimisation\n            η=1e-1,\n            hooks=[StopWhen((a; kws...) -> norm(x(a) - kws[:z]) < 1e-6)],  # hook stops opt when residual is below 1e-6.\n        )\njulia> sol = minimize!(f, a)  # Optimise\njulia> @show SemioticOpt.x(sol)  # Print out the optimal value\n2-element Vector{Float64}:\n 3.4163644416613304e-6\n 1.6909278549636878e-6","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nAs a rule, we recommend you use an accessor function rather than accessing struct fields directly when using this package. That will lend itself to greater stability in case we change stuff internally. As an example, prefer to use x(a) as compared with a.x, where a is the GradientDescent struct used in the example before. Similarly, for setting, prefer x(a, v) to a.x = v. Not only does this pattern aid in stability, but also in functionality such as handling setting for immutable structs.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Make sure you've installed Julia 1.8 or greater. From the Julia REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\nPkg> add https://github.com/semiotic-ai/SemioticOpt.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"To specify a version, use","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\nPkg> add https://github.com/semiotic-ai/SemioticOpt.jl#v0.1.0","category":"page"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"This section will introduce you to the basic workflow that you'll use to optimise functions, as well as the algorithms we've implemented.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Generally speaking, there are two parts to optimising. The first is to specify the function you want to optimise. Say,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> f(x) = sum(x .^ 2)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Then, you want to create a Hook with the SemioticOpt.IsStoppingCondition trait. Else, optimisation will get stuck in an infinite loop.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using LinearAlgebra\njulia> h = StopWhen((a; locals...) -> norm(x(a) - locals[:z]) < 1e-6)  # Stop when the residual is less than the tolerance","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Then, you want to choose the algorithm you want to use for minimisation and specify its parameters.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> a = GradientDescent(; x=[100.0, 50.0], η=1e-1, hooks=[h,])  # Specify parameters for optimisation","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Finally, you'll run minimize! or minimize.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> sol = minimize!(f, a)  # Optimise","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SemioticOpt.minimize!\nSemioticOpt.minimize","category":"page"},{"location":"algorithms/#SemioticOpt.minimize!","page":"Algorithms","title":"SemioticOpt.minimize!","text":"minimize!(f::Function, a::OptAlgorithm)\n\nMinimize f using a.\n\nDoes in-place updates of a.x. This will generally be more performant than SemioticOpt.minimize. However, there are cases in which this will be worse, so we provide both.\n\nwarning: Warning\nIf you don't provide any hook with the IsStoppingCondition trait, this will loop forever.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#SemioticOpt.minimize","page":"Algorithms","title":"SemioticOpt.minimize","text":"minimize(f::Function, a::OptAlgorithm)\n\nMinimize f using a.\n\nThis will generally be less performant than SemioticOpt.minimize!. However, there are cases in which this will be better, so we provide it as an option.\n\nwarning: Warning\nIf you don't provide any hook with the IsStoppingCondition trait, this will loop forever.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"sol here is a struct containing various metadata. If you only care about the optimal value of x, then grab it using","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> SemioticOpt.x(sol)","category":"page"},{"location":"algorithms/#Gradient-Descent","page":"Algorithms","title":"Gradient Descent","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"From Wikipedia:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[Gradient Descent]... is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"If the function we're optimising f is convex, then a local minimum is also a global minimum. The update rule for gradient descent is x_n+1=x_n - ηf(x_n).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SemioticOpt.GradientDescent","category":"page"},{"location":"algorithms/#SemioticOpt.GradientDescent","page":"Algorithms","title":"SemioticOpt.GradientDescent","text":"GradientDescent(;x::V, η::T, hooks::S) where {\n    T<:Real,V<:AbstractVector{T},S<:AbstractVecOrTuple{<:Hook}\n}\n\nParameters for gradient descent learning.\n\nFields\n\nη::T is the learning rate/step size.\nx::V is the current best guess for the solution.\nhooks::S are the hooks\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#A-Quick-Interlude:-Choosing-Step-Size","page":"Algorithms","title":"A Quick Interlude: Choosing Step Size","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"If your function is L-Lipschitz and the optimal pointf^*  -infty, then eta  2  L will converge to the optimal point. Note that eta is the step size. See Theorem 4.2 for details. So that you don't have to remember this relationship, just know that if you can tell us the Lipschitz constant of the function you're trying to minimise, we'll tell you what the step size is. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SemioticOpt.stepsize","category":"page"},{"location":"algorithms/#SemioticOpt.stepsize","page":"Algorithms","title":"SemioticOpt.stepsize","text":"stepsize(l::Real)\n\nReturn the optimal step size for a convex function with Lipschitz constant l.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Projected-Gradient-Descent","page":"Algorithms","title":"Projected Gradient Descent","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Projected Gradient Descent is a more general gradient descent in a sense. Whereas gradient descent itself has no constraint, projected gradient descent allows for you to specify a constraint. In particular, here, we are interested in solving the problem of min_x f(x) subject to some constraints. Those constraints define the feasible region mathcalC. Thus, the full problem we're trying to solve is min_x f(x) textrmsubject to  xinmathcalC. Once we do the standard gradient descent step, here we project the result onto the feasible set. In other words, ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"beginalign*\n    y_n = x_n - ηf(x_n) \n    x_n+1 = textrmPr(y_n)_mathcalC\nendalign*","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To use PGD, you'll need to specify t, the projection function. Any projection function must take only x as input. To get more complex behaviour, you may find it useful to leverage currying (partial function application).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Let's look at an example of this. Say we want to minimise f(x)=sum x^2 subject to a unit simplex constraint. We provide a function SemioticOpt.σsimplex. However, this function doesn't only take x as input, but also takes σ. In order to create a projection function in terms of only x, we will apply currying by creating a new function.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using SemioticOpt\njulia> unitsimplex(x) = σsimplex(x, 1)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Now, we can create a PGD parameters struct and solve our problem.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using LinearAlgebra\njulia> f(x) = sum(x.^2)\njulia> a = ProjectedGradientDescent(;\n            x=[100.0, 50.0],\n            η=1e-1,\n            hooks=[StopWhen((a; kws...) -> norm(x(a) - kws[:z]) < 1e-6)],\n            t=unitsimplex,\n        )\njulia> aopt = minimize(f, a)\njulia> SemioticOpt.x(aopt)\n2-element Vector{Float64}:\n 0.5000023384026198\n 0.49999766159738035","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SemioticOpt.ProjectedGradientDescent","category":"page"},{"location":"algorithms/#SemioticOpt.ProjectedGradientDescent","page":"Algorithms","title":"SemioticOpt.ProjectedGradientDescent","text":"ProjectedGradientDescent(;\n    x::AbstractVector{T}, η::T, hooks::AbstractVecOrTuple{<:Hook}, t::F\n) where {T<:Real,F<:Function}\nProjectedGradientDescent(g::G, t::F) where {G<:GradientDescent,F<:Function}\n\nSpecifies parameters for SemioticOpt.GradientDescent and the projection function t.\n\nt takes as input the vector to be projected x and returns the projected vector.\n\nForwarded Methods\n\nSemioticOpt.x\nSemioticOpt.η\nSemioticOpt.hooks\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Predefined-Projection-Functions","page":"Algorithms","title":"Predefined Projection Functions","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SemioticOpt.σsimplex\nSemioticOpt.gssp","category":"page"},{"location":"algorithms/#SemioticOpt.σsimplex","page":"Algorithms","title":"SemioticOpt.σsimplex","text":"σsimplex(x::AbstractVector{T}, σ::Real) where {T<:Real}\n\nProject x onto the σ-simplex.\n\nIn other words, project xs to be non-negative and sum to σ.\n\nThis operation is precision-senstive, so we conver the data to bigfloat within the function. We then convert back to T before returning.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#SemioticOpt.gssp","page":"Algorithms","title":"SemioticOpt.gssp","text":"gssp(x::AbstractVector{<:Real}, k::Int, σ::Real)\n\nProject x onto the intersection of the set of k-sparse vectors and the σ-simplex.\n\nReference: http://proceedings.mlr.press/v28/kyrillidis13.pdf\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Halpern-Iteration","page":"Algorithms","title":"Halpern Iteration","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Halpern iteration is a method of anchoring an iterative optimisation algorithm to some anchor point x_0. The general form of the algorithm is given by","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"x_k+1 = lambda_k+1x_0 + (1 - lambda_k+1)T(x_k)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Here, T mathbbR^d to mathbbR^d is a non-expansive map (i.e., forall xy in mathbbR^d T(x) - T(y) leq x - y). lambda_k is a step size that must be chosen such that it satisfies the properties.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"beginalign*\n    lim_ktoinfty lambda_k = 0 \n    sum_k=1^infty lambda_k = infty \n    sum_k=1^infty lambda_k+1 - lambda_k  infty \nendalign*","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A reasonable first guess for lambda_k might be frac1k if you're unsure about what to pick.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"note: Note\nIn the formula for Halpern Iteration, we actually use k+1. In our code, you should still define lambda_k as we'll add the one for you.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"This is an implicitly regularised method. If you use an algorithm like gradient descent with Halpern Iteration, you'll converge to the solution with the minimum ell2 distance from x₀.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Implementation-wise, we've actually defined Halpern Iteration using a Hook that exhibits the SemioticOpt.RunAfterIteration trait. To use this, you'll want to add this hook to an existing algorithm. For example,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"julia> using LinearAlgebra\njulia> f(x) = sum(x.^2)\njulia> a = GradientDescent(;\n            x=[100.0, 50.0],\n            η=1e-1,\n            hooks=[\n                StopWhen((a; kws...) -> norm(x(a) - kws[:z]) < 1e-6),\n                HalpernIteration(; x₀=[10, 10], λ=k -> 1 / k),\n            ],\n        )\njulia> o = minimize!(f, a)\njulia> x(o)\n2-element Vector{Float64}:\n 0.005945303210463734\n 0.005945303210463734","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SemioticOpt.HalpernIteration","category":"page"},{"location":"algorithms/#SemioticOpt.HalpernIteration","page":"Algorithms","title":"SemioticOpt.HalpernIteration","text":"HalpernIteration{T<:Real,V<:AbstractVector{T}} <: Hook\n\nA hook for using Halpern Iteration. in which you should specify the x₀ and λ.\n\n\n\n\n\n","category":"type"}]
}
